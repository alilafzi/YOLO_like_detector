# YOLO_like_detector

## Project description
The goal of this project is to perform multi-object detection and localization in COCO images using an approach similar to YOLO that will be described below. We will be working with bus, car, and truck as our 3 classes of interest. We only select images that have a maximum of 5 instances of these objects and contain at least 2 of these classes or all 3 of them. The selected images are then resized to 128x128, and all the corresponding annotations are modified accordingly. <br>

## Training
The YOLO detector is a regression-based network, i.e. it does both detection and localization of objects in the image based on some regression-based metrics. The training function in which the network is trained has many differences from the ones for single object purposes. We first discretize the image using a grid of 36 (6x6) cells. Our next goal is to assign an anchor box to each of the object instances based on its aspect ratio. To this end, we assume 5 different anchor boxes with aspect ratios of 1/5, 1/3, 1, 3, and 5, and assign the anchor box that has the closest value to the aspect ratio of each object. For each anchor box, we assume a vector with 8 elements (YOLO vector) of all zeros initially. The last 3 elements denote the class of the object the assigned anchor box refers to in a one-hot encoding manner. The first element of this vector would be 1 for the assigned anchor box. Consequently, there will be a total of 36x5 of these 8-element vectors. After reshaping row-wise, we get a giant vector with 1440 (36x5x8) elements, which we name the "flattened YOLO tensor". The center of each object is calculated based on which, we determine which cell it belongs to. We then find the difference between the object center and the center of its assigned cell in both x and y directions in terms of the cell dimensions and put them as the second and third elements of the YOLO vector whose first element is 1. Similarly, we replace the fourth and fifth elements of such YOLO vector with the true height and width of the object bounding box in terms of the cell dimensions. Now, the goal of the trained network is to create a predicted vector as close as possible to this giant YOLO tensor. A custom loss function is used for training the network. This loss is the summation of the MSE loss for the first 5 elements of each 8-element YOLO vector (inside the whole 1440-element flattened YOLO tensor) and the Cross Entropy loss for the last 3 elements of that vector, corresponding to the classes. The implementation of this loss can be found between lines 690 and 727 of "hw06_training" notebook. It is important to note that the MSE loss for the entire output and YOLO tensor was also used, but this custom loss provided slightly better results. <br>

## Validation
After loading the trained network and applying it to the validation data, it is crucial to come up with some strategy to determine which predicted bounding boxes should be chosen from the unscrambled output tensor for each corresponding ground truth box. 3 different strategies were tested for doing so: <br><br>
•	Extract the most probable bounding boxes from the 1440-element output based on the larger first element in their 8-element YOLO vector. For example, if the image contains 2 ground truth bounding boxes, the 2 bounding boxes with larger first elements among all the other ones are selected as the boxes responsible for predicting the objects inside the ground truth ones. This strategy is based on the logic that the first elements in each YOLO vector denote the confidence of the algorithm that an object presents in that bounding box.
•	The shortcoming of the previous strategy is that it does not guarantee that the determined boxes predict the class label of the object correctly. Consequently, it was attempted to follow the logic of eq. 1 in the original YOLO 1 paper (https://arxiv.org/abs/1506.02640). In other words, a combination of the first element and the larger of the last 3 elements (as the confidence of the algorithm about the predicted class) was utilized as the most probable bounding boxes selection criteria. However, this led to worse results for us in practice. <br>
•	The last strategy makes the most sense. Different true class labels and their corresponding ground truth boxes of the objects in each image are grouped together, as well as all the 180 (1440/8) prediction boxes based on their labels (argmax of last 3 elements in each vector), respectively. Then, depending on the number of each label category, the most probable predicted bounding boxes are extracted from the unscrambled output tensor using the first mentioned criterion (first bullet point above) for each group separately. Nevertheless, this technique also ended up in worse results for our case. <br><br>
Based on the provided explanation, the first strategy is implemented for extracting the most probable predicted bounding boxes. The detail of its implementation is the following: <br><br>
•	Between lines 673 and 681, the true assigned anchor boxes from the previous step (having a first element of 1) are selected. <br>
•	Between lines 706 and 732, the cell indices associated with the determined assigned boxes are found, and after getting the cell row and column indices from them, the x and y center of each ground truth box is derived by calculating the center of the assigned YOLO cell and using the offsets provided as del-x and del-y (the second and third element of the YOLO vector) multiplied by the YOLO interval (Lines 720 to 724). After obtaining the center coordinates, it is time to map that YOLO vector back to the image; the x and y of the top left corner and w and h of the box are determined in lines 726 to 732. It is crucial to mention that the retrieved ground truth bounding boxes with this procedure match those provided originally by the data loader. <br>
•	Between lines 683 and 704, the indices (between 0 and 179) associated with the most probable predictory bounding boxes based on the aforementioned strategy are determined. <br>
•	After obtaining these indices, similar to the procedure for the ground truth cases, the predicted bounding boxes are also mapped back for each image. This is done in lines 734 to 761. <br>

## Reference:
https://engineering.purdue.edu/kak/distDLS/

